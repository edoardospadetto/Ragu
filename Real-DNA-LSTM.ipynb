{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM on real DNA sequences\n",
    "\n",
    "# Leggi qui\n",
    "Nella parte prima di START HERE ho commentato le linee che non servono più perché ho già generato i file necessari, che puoi trovare nella mia cartella /ubuntu (non dovrebbero esserci problemi di permessi). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "from keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout\n",
    "from keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2903159</td>\n",
       "      <td>2915884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2932446</td>\n",
       "      <td>2972497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2960608</td>\n",
       "      <td>2996556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2960608</td>\n",
       "      <td>2999518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chr1</td>\n",
       "      <td>2960608</td>\n",
       "      <td>3001253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0        1        2\n",
       "0  chr1  2903159  2915884\n",
       "1  chr1  2932446  2972497\n",
       "2  chr1  2960608  2996556\n",
       "3  chr1  2960608  2999518\n",
       "4  chr1  2960608  3001253"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intr_coords = pd.read_csv('Akey_intr_coords.bed', header = None, sep = \"\\t\")\n",
    "intr_coords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "intr_lengths = intr_coords.iloc[:, 2]-intr_coords.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DescribeResult(nobs=83601, minmax=(10002, 1194940), mean=92137.5803877944, variance=6139175379.516866, skewness=2.7095069302166377, kurtosis=11.38558261567325)\n"
     ]
    }
   ],
   "source": [
    "print(stats.describe(intr_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!zcat hg19.fa.gz | bgzip -c > hg19.fa.bgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10000 Neanderthal introgressed haplotypes\n",
      "Finished 20000 Neanderthal introgressed haplotypes\n",
      "Finished 30000 Neanderthal introgressed haplotypes\n",
      "Finished 40000 Neanderthal introgressed haplotypes\n",
      "Finished 50000 Neanderthal introgressed haplotypes\n",
      "Finished 60000 Neanderthal introgressed haplotypes\n",
      "Finished 70000 Neanderthal introgressed haplotypes\n",
      "Finished 80000 Neanderthal introgressed haplotypes\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "with open('hg19_intr_regions.fa', 'a') as fp:\n",
    "    for i in range(intr_coords.shape[0]):\n",
    "        coord = str(str(intr_coords.iloc[i, 0]) + ':' \n",
    "                    + str(intr_coords.iloc[i, 1]) + '-' + str(intr_coords.iloc[i, 2]))\n",
    "        subprocess.run(['samtools', 'faidx', 'hg19.fa.bgz', str(coord)], stdout = fp)\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal introgressed haplotypes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr_sizes = pd.read_csv(\"hg19.fa.bgz.fai\", header = None, sep = \"\\t\")\\nchr_sizes = chr_sizes.drop([2, 3, 4], axis = 1)\\nchr_sizes.head()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"chr_sizes = pd.read_csv(\"hg19.fa.bgz.fai\", header = None, sep = \"\\t\")\n",
    "chr_sizes = chr_sizes.drop([2, 3, 4], axis = 1)\n",
    "chr_sizes.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chr_list = []\\nstart_list = []\\nend_list = []\\nintr_lengths = list(intr_coords.iloc[:, 2] - intr_coords.iloc[:, 1])\\na = 0\\nfor i in range(intr_coords.shape[0]):\\n    chr_df = intr_coords[intr_coords[0].isin([intr_coords.iloc[i,0]])]\\n    overlap = True\\n    while overlap == True:\\n        reg_start = np.random.randint(1, int(chr_sizes[chr_sizes[0] == intr_coords.iloc[i,0]].iloc[:,1]))\\n        reg_end = reg_start + intr_lengths[i]\\n        for j in range(chr_df.shape[0]):\\n            b1 = chr_df.iloc[j,1]\\n            b2 = chr_df.iloc[j,2]\\n            if (reg_start > b1 and reg_start < b2) or (reg_end > b1 and reg_end < b2) or             (b1 > reg_start and b1 < reg_end) or (b2 > reg_start and b2 < reg_end):\\n                overlap = True\\n                break\\n            else:\\n                overlap = False\\n    chr_list.append(intr_coords.iloc[i,0])\\n    start_list.append(reg_start)\\n    end_list.append(reg_end)\\n    a = a + 1\\n    if a%10000 == 0:\\n            print(\\'Finished \\' + str(a) + \\' Neanderthal introgressed haplotypes\\')\\ndepl_coords = pd.DataFrame({\\'0\\': chr_list, \\'1\\': start_list, \\'2\\': end_list})\\ndepl_coords.to_csv(\"Akey_depl_coords.bed\", index = False, header = False, sep = \"\\t\")\\ndepl_coords.head()'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"chr_list = []\n",
    "start_list = []\n",
    "end_list = []\n",
    "intr_lengths = list(intr_coords.iloc[:, 2] - intr_coords.iloc[:, 1])\n",
    "a = 0\n",
    "for i in range(intr_coords.shape[0]):\n",
    "    chr_df = intr_coords[intr_coords[0].isin([intr_coords.iloc[i,0]])]\n",
    "    overlap = True\n",
    "    while overlap == True:\n",
    "        reg_start = np.random.randint(1, int(chr_sizes[chr_sizes[0] == intr_coords.iloc[i,0]].iloc[:,1]))\n",
    "        reg_end = reg_start + intr_lengths[i]\n",
    "        for j in range(chr_df.shape[0]):\n",
    "            b1 = chr_df.iloc[j,1]\n",
    "            b2 = chr_df.iloc[j,2]\n",
    "            if (reg_start > b1 and reg_start < b2) or (reg_end > b1 and reg_end < b2) or \\\n",
    "            (b1 > reg_start and b1 < reg_end) or (b2 > reg_start and b2 < reg_end):\n",
    "                overlap = True\n",
    "                break\n",
    "            else:\n",
    "                overlap = False\n",
    "    chr_list.append(intr_coords.iloc[i,0])\n",
    "    start_list.append(reg_start)\n",
    "    end_list.append(reg_end)\n",
    "    a = a + 1\n",
    "    if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal introgressed haplotypes')\n",
    "depl_coords = pd.DataFrame({'0': chr_list, '1': start_list, '2': end_list})\n",
    "depl_coords.to_csv(\"Akey_depl_coords.bed\", index = False, header = False, sep = \"\\t\")\n",
    "depl_coords.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "depl_coords = pd.read_csv(\"Akey_depl_coords.bed\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n"
     ]
    }
   ],
   "source": [
    "!bedtools intersect -a Akey_intr_coords.bed -b Akey_depl_coords.bed | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10000 Neanderthal ancestry depleted regions\n",
      "Finished 20000 Neanderthal ancestry depleted regions\n",
      "Finished 30000 Neanderthal ancestry depleted regions\n",
      "Finished 40000 Neanderthal ancestry depleted regions\n",
      "Finished 50000 Neanderthal ancestry depleted regions\n",
      "Finished 60000 Neanderthal ancestry depleted regions\n",
      "Finished 70000 Neanderthal ancestry depleted regions\n",
      "Finished 80000 Neanderthal ancestry depleted regions\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "with open('hg19_depl_regions.fa', 'a') as fp:\n",
    "    for i in range(depl_coords.shape[0]):\n",
    "        coord = str(str(depl_coords.iloc[i, 0]) + ':' \n",
    "                    + str(depl_coords.iloc[i, 1]) + '-' + str(depl_coords.iloc[i, 2]))\n",
    "        subprocess.run(['samtools', 'faidx', 'hg19.fa.bgz', str(coord)], stdout = fp)\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' Neanderthal ancestry depleted regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: hg19_intr_regions.fa: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c N hg19_intr_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18502152\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c N hg19_depl_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 10000 entries\n",
      "Finished 20000 entries\n",
      "Finished 30000 entries\n",
      "Finished 40000 entries\n",
      "Finished 50000 entries\n",
      "Finished 60000 entries\n",
      "Finished 70000 entries\n",
      "Finished 80000 entries\n",
      "We have processed 83601 entries and written 73364 entries to two fasta-files\n"
     ]
    }
   ],
   "source": [
    "intr_file = 'hg19_intr_regions.fa'\n",
    "depl_file = 'hg19_depl_regions.fa'\n",
    "a = 0\n",
    "i = 0\n",
    "with open('hg19_intr_clean.fa', 'a') as intr_out, open('hg19_depl_clean.fa', 'a') as depl_out:\n",
    "    for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "        upper_intr = intr.seq.upper()\n",
    "        upper_depl = depl.seq.upper()\n",
    "        a = a + 1\n",
    "        if a%10000 == 0:\n",
    "            print('Finished ' + str(a) + ' entries')\n",
    "        if 'N' not in str(upper_intr) and 'N' not in str(upper_depl):\n",
    "            intr.seq = upper_intr\n",
    "            SeqIO.write(intr, intr_out, 'fasta')\n",
    "            depl.seq = upper_depl\n",
    "            SeqIO.write(depl, depl_out, 'fasta')\n",
    "            i = i + 1\n",
    "        else:\n",
    "            continue\n",
    "print('We have processed ' + str(a) + ' entries and written ' + str(i) + ' entries to two fasta-files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "863\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c N hg19_intr_regions.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18502152\r\n"
     ]
    }
   ],
   "source": [
    "!grep -c N hg19_depl_regions.fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START HERE\n",
    "\n",
    "Qui il codice commentato è perché volevo fare un po' di prove, quindi può essere cambiato a piacimento\n",
    "\n",
    "Attenzione perché la rete così com'è ci mette tanto ad allenarsi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 20000 entries\n",
      "Finished 40000 entries\n",
      "Finished 60000 entries\n"
     ]
    }
   ],
   "source": [
    "intr_file = 'hg19_intr_clean.fa'\n",
    "depl_file = 'hg19_depl_clean.fa'\n",
    "\n",
    "\"\"\"a = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    cut = 1000\n",
    "    if len(str(intr.seq)) < cut or len(str(depl.seq)) < cut:\n",
    "        continue\n",
    "    s_intr = str(intr.seq)[0:cut]\n",
    "    s_depl = str(depl.seq)[0:cut]\n",
    "    if s_intr.count('A')>0 and s_intr.count('C')>0 and s_intr.count('G')>0 and s_intr.count('T')>0 and \\\n",
    "    s_depl.count('A')>0 and s_depl.count('C')>0 and s_depl.count('G')>0 and s_depl.count('T')>0:\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "    a = a + 1\n",
    "    if a%10000 == 0:\n",
    "        print('Finished ' + str(a) + ' entries')\"\"\"\n",
    "\n",
    "e = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    \n",
    "    #cutoff = 200\n",
    "    #my_intr_seq = str(intr.seq)[0:cutoff]\n",
    "    #my_depl_seq = str(depl.seq)[0:cutoff]\n",
    "    #intr_seqs.append(my_intr_seq)\n",
    "    #depl_seqs.append(my_depl_seq)\n",
    "    \n",
    "    step = 200; jump = 1; a = 0; b = step; n_jumps = 5\n",
    "    for j in range(n_jumps):\n",
    "        s_intr = str(intr.seq)[a:b]\n",
    "        s_depl = str(depl.seq)[a:b]\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "        a = a + jump\n",
    "        b = a + step\n",
    "    \n",
    "    e = e + 1\n",
    "    if e%20000 == 0:\n",
    "        print('Finished ' + str(e) + ' entries')\n",
    "        \n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "kmer = 10\n",
    "intr_texts = [' '.join(getKmers(i, kmer)) for i in intr_seqs]\n",
    "depl_texts = [' '.join(getKmers(i, kmer)) for i in depl_seqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733640"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sequences = intr_seqs + depl_seqs\n",
    "len(sequences)\"\"\"\n",
    "\n",
    "merge_texts = intr_texts + depl_texts\n",
    "len(merge_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "733640\n"
     ]
    }
   ],
   "source": [
    "\"\"\"labels = list(np.ones(len(intr_seqs))) + list(np.zeros(len(depl_seqs)))\n",
    "len(labels)\"\"\"\n",
    "\n",
    "labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e = 0\\nintr_seqs = []\\ndepl_seqs = []\\nfor intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\\n    \\n    cutoff = 200\\n    my_intr_seq = str(intr.seq)[0:cutoff]\\n    my_depl_seq = str(depl.seq)[0:cutoff]\\n    \\n    intr_seqs.append(my_intr_seq)\\n    depl_seqs.append(my_depl_seq)\\n    \\n    e = e + 1\\n    if e%20000 == 0:\\n        print('Finished ' + str(e) + ' entries')\\n        \\ndef getKmers(sequence, size):\\n    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\\n\\nkmer = 10\\nintr_texts = [' '.join(getKmers(i, kmer)) for i in intr_seqs]\\ndepl_texts = [' '.join(getKmers(i, kmer)) for i in depl_seqs]\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"e = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    \n",
    "    cutoff = 200\n",
    "    my_intr_seq = str(intr.seq)[0:cutoff]\n",
    "    my_depl_seq = str(depl.seq)[0:cutoff]\n",
    "    \n",
    "    intr_seqs.append(my_intr_seq)\n",
    "    depl_seqs.append(my_depl_seq)\n",
    "    \n",
    "    e = e + 1\n",
    "    if e%20000 == 0:\n",
    "        print('Finished ' + str(e) + ' entries')\n",
    "        \n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "kmer = 10\n",
    "intr_texts = [' '.join(getKmers(i, kmer)) for i in intr_seqs]\n",
    "depl_texts = [' '.join(getKmers(i, kmer)) for i in depl_seqs]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merge_texts = intr_texts + depl_texts\\nlen(merge_texts)'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"merge_texts = intr_texts + depl_texts\n",
    "len(merge_texts)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\\nprint(len(labels))'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\n",
    "print(len(labels))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153634 568397 382160 ... 635443 506832 658088]\n",
      " [568397 382160 380712 ... 506832 658088 654350]\n",
      " [382160 380712 323461 ... 658088 654350 503434]\n",
      " ...\n",
      " [  1173    900    397 ...  64841  93299 115195]\n",
      " [   900    397    537 ...  93299 115195 135311]\n",
      " [   397    537   1117 ... 115195 135311  63747]]\n",
      "\n",
      "\n",
      "(733640, 191)\n"
     ]
    }
   ],
   "source": [
    "#cv = CountVectorizer()\n",
    "#X = cv.fit_transform(merge_texts)\n",
    "\n",
    "#tfidf_transformer = TfidfTransformer()\n",
    "#X = tfidf_transformer.fit_transform(X)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(merge_texts)\n",
    "#X = tokenizer.texts_to_matrix(merge_texts, mode = 'freq')\n",
    "\n",
    "encoded_docs = tokenizer.texts_to_sequences(merge_texts)\n",
    "max_length = max([len(s.split()) for s in merge_texts])\n",
    "X = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')\n",
    "\n",
    "print(X)\n",
    "print('\\n')\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size = 0.20, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586912, 191)\n",
      "(146728, 191)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586912, 191)\n",
      "(146728, 191)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = X_train[:200000]\n",
    "X_test1 = X_test[:5000]\n",
    "y_train1 = y_train[:200000]\n",
    "y_test1 = y_test[:5000]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "max_length = max([len(s.split()) for s in merge_texts])\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944238\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model = Sequential()\\n#model.add(Dense(3000, input_shape = (X.shape[1],), activation = \\'sigmoid\\'))\\nmodel.add(Embedding(vocab_size, 32, input_length = max_length, dropout = 0.2))\\n#model.add(Conv1D(filters = 16, kernel_size = 5, padding = \\'same\\', activation = \\'relu\\'))\\n#model.add(MaxPooling1D(pool_size = 2))\\n#model.add(LSTM(10)) #dropout = 0.2, recurrent_dropout = 0.2\\n#model.add(GRU(10))\\nmodel.add(SimpleRNN(10, dropout = 0.2, recurrent_dropout = 0.2))\\n#model.add(Flatten())\\n#model.add(Dense(10, activation = \\'sigmoid\\'))\\nmodel.add(Dense(1, activation = \\'sigmoid\\'))\\n\\nepochs = 5\\nmodel.compile(loss = \\'binary_crossentropy\\', optimizer = \\'rmsprop\\', metrics = [\\'accuracy\\'])\\n#model.compile(loss = \\'binary_crossentropy\\', optimizer = \\'adam\\', metrics = [\\'accuracy\\'])\\n#model.compile(loss = \\'binary_crossentropy\\', optimizer = \\'SGD\\', metrics = [\\'accuracy\\'])\\n#model.compile(loss = \\'binary_crossentropy\\', optimizer = RMSprop(lr = 0.0001), metrics = [\\'accuracy\\'])\\ncheckpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = \\'val_acc\\', verbose = 1, \\n                             save_best_only = True, mode = \\'max\\')\\nprint(model.summary())'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"model = Sequential()\n",
    "#model.add(Dense(3000, input_shape = (X.shape[1],), activation = 'sigmoid'))\n",
    "model.add(Embedding(vocab_size, 32, input_length = max_length, dropout = 0.2))\n",
    "#model.add(Conv1D(filters = 16, kernel_size = 5, padding = 'same', activation = 'relu'))\n",
    "#model.add(MaxPooling1D(pool_size = 2))\n",
    "#model.add(LSTM(10)) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "#model.add(GRU(10))\n",
    "model.add(SimpleRNN(10, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(10, activation = 'sigmoid'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "epochs = 5\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.0001), metrics = ['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = 'val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max')\n",
    "print(model.summary())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 10)          9442380   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 20)                1680      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 9,444,281\n",
      "Trainable params: 9,444,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10)) #dropout = 0.2 #input_length = max_length\n",
    "#model.add(Conv1D(filters = 16, kernel_size = 5, padding = 'same', activation = 'relu'))\n",
    "#model.add(MaxPooling1D(pool_size = 2))\n",
    "#model.add(LSTM(10)) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "model.add(Bidirectional(LSTM(10))) #dropout = 0.2, recurrent_dropout = 0.2\n",
    "#model.add(Bidirectional(SimpleRNN(10)))\n",
    "#model.add(GRU(10))\n",
    "#model.add(SimpleRNN(10, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(10, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "epochs = 5\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = 'SGD', metrics = ['accuracy'])\n",
    "#model.compile(loss = 'binary_crossentropy', optimizer = RMSprop(lr = 0.0001), metrics = ['accuracy'])\n",
    "checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = 'val_acc', verbose = 1, \n",
    "                             save_best_only = True, mode = 'max')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 40000 samples\n",
      "Epoch 1/5\n",
      "   576/160000 [..............................] - ETA: 35:13 - loss: 0.6925 - accuracy: 0.5278"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-07071a593f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit(X_train1, y_train1, \n\u001b[1;32m      2\u001b[0m                     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                     callbacks = [checkpoint])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train1, y_train1, \n",
    "                    epochs = epochs, verbose = 1, validation_split = 0.2, batch_size = 32, shuffle = True, \n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plt.figure()\n",
    "\n",
    "predicted_labels = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, [np.round(i[0]) for i in predicted_labels])\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, cmap = plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix', fontsize = 15)\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label', fontsize = 15)\n",
    "plt.ylabel('Predicted label', fontsize = 15)\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment = 'center', verticalalignment = 'center', fontsize = 20,\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')\n",
    "plt.show()\"\"\"\n",
    "\n",
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "predicted_labels = model.predict(X_test1)\n",
    "cm = confusion_matrix(y_test1, [np.round(i[0]) for i in predicted_labels])\n",
    "print('Confusion matrix:\\n',cm)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "plt.imshow(cm, cmap = plt.cm.Blues)\n",
    "plt.title('Normalized confusion matrix', fontsize = 20)\n",
    "plt.colorbar()\n",
    "plt.xlabel('True label', fontsize = 20)\n",
    "plt.ylabel('Predicted label', fontsize = 20)\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], '.2f'),\n",
    "             horizontalalignment = 'center', verticalalignment = 'center', fontsize = 20,\n",
    "             color='white' if cm[i, j] > 0.5 else 'black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test1, y_test1, verbose = 0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
